# Visual Proof Pack — Overview

## What is the Visual Proof Pack?

The **Visual Proof Pack** is a validation artifact that provides visual evidence of component detection accuracy. It consists of:

- **Annotated screenshots**: Live web pages with colored bounding boxes highlighting detected components
- **Manifest JSON files**: Structured data with bbox coordinates, locators, and component metadata
- **Human labels**: QA operator judgments (confirm/wrong/skip) for each detection

The proof pack serves as:

1. **Validation checkpoint**: Verify detector precision before running full analysis
2. **Debugging tool**: Visual inspection of what the detectors actually find
3. **Documentation**: Concrete examples for stakeholders and future developers

---

## Workflow

```text
DB Export → JSON → Proof Runner → Annotated Screenshots → Human QA → Labels → Regression Gates → Analysis
```

### Step 1: DB Export

Export detections from `david_component_usage` table for 5 proof pages (2 VLP + 3 Editorial).

**Input**: SQLite DB (`api/data/cu-tool.db`)
**Output**: `analysis/artifacts/visual-proof/detections.json`

### Step 2: Proof Runner (Live)

Load JSON, navigate to live URLs, query DOM using locator strategy, inject overlays, capture screenshots.

**Input**: `detections.json`
**Output**:

- `<slug>.annotated.png` (full-page screenshot with bounding boxes)
- `<slug>.manifest.json` (bbox coordinates + locator metadata)

### Step 3: Human QA

QA operator labels each detection as confirm/wrong/skip using keyboard shortcuts.

**Input**: Manifest JSON + annotated screenshots
**Output**: `labels.jsonl` (append-only, one JSON object per line)

### Step 4: Regression Gates

Compute precision per component type, validate against thresholds, block analysis if gates fail.

**Input**: `labels.jsonl`
**Output**: `regression-report.md` (pass/fail per gate)

---

## Inputs

### From Database

- **URL inventory**: 5 proof pages filtered from `url_inventory` (UK market, fetched successfully)
- **Component detections**: Per-URL detections from `david_component_usage` table
- **Evidence strings**: Parsed into structured data (item counts, selectors, controls)

> **Note**: Proof-pack URLs must match `url_inventory.url` exactly. The inventory may contain redirect URLs (e.g., `owners.html`), while proof-pack lists may reference canonical URLs (e.g., `ownership.html` after 301/302 redirect). Current exporter requires exact match to `url_inventory.url`. Redirect/canonical resolution via `final_url`/`canonical_url` is future work (TH-01.1).

### Page Lists

- `analysis/visual-proof/pages.vlp.txt`: 2 VLP URLs (Juke, Ariya)
- `analysis/visual-proof/pages.editorial.txt`: 3 Editorial URLs (Electric Vehicles, Ownership, Pop-up Event)

### Locator Strategy

- Component-specific CSS selector patterns (documented in `runner.md`)
- Global chrome exclusion rules (header, nav, footer, dialogs, OneTrust)

---

## Outputs

### Artifacts Directory Structure

```text
analysis/artifacts/visual-proof/
├── detections.json           # DB export (5 URLs, all component instances)
├── full/                     # Proof runner outputs
│   ├── juke/
│   │   ├── juke.annotated.png
│   │   └── juke.manifest.json
│   ├── ariya/
│   ├── electric-vehicles/
│   ├── ownership/
│   └── pop-up-event/
├── labels.jsonl              # Human QA labels (append-only)
├── regression-report.md      # Gate validation results
└── detector-changes.md       # Before/after hardening diff
```

### What is Committed to Repo

✅ **Committed**:

- Page lists (`pages.vlp.txt`, `pages.editorial.txt`, `pages.homepage.txt`)
- Documentation (all `.md` files in `docs/visual-proof/`)
- Scripts and runner logic (`analysis/scripts/`, `analysis/visual-proof/runner/`)

❌ **NOT Committed** (local-only, reproducible):

- `detections.json` — Generated from SQLite DB
- `*.manifest.json` — Generated by proof runner
- `*.annotated.png` — Screenshots (~500KB–2MB each)
- `labels.jsonl` — Human QA labels
- `regression-report.md`, `detector-changes.md` — Analysis outputs
- SQLite DB (`api/data/cu-tool.db`)
- HTML cache (`api/data/html/UK/`)

Rationale: Generated artifacts are reproducible by re-running the pipeline. Large binaries (screenshots, DB, HTML cache) are unsuitable for git.

---

## Key Properties

### Deterministic

- Fixed viewport (1920x1080)
- Fixed component list (from detections.json, not heuristic re-detection)
- Fixed locator strategy (documented, versioned)

### Reproducible

- Screenshots can be regenerated by re-running runner with same inputs
- JSON artifacts are deterministic (no random UUIDs, timestamps are for metadata only)

### Auditable

- Every detection has visual evidence (screenshot) + structured data (manifest JSON)
- Human labels have timestamps and are append-only (no edits, no deletes)

### Transparent

- Locator strategy documented (no black box)
- Bbox coordinates in document coordinate system (reproducible)
- Global chrome exclusions explicit (header/nav/footer/dialogs)

---

## Non-Goals

- **Not a full-site audit**: Proof pack covers 5 pages only, not all 8,468 URLs
- **Not automated regression testing**: Screenshots are for human QA, not pixel-perfect comparison
- **Not ML training data**: Labels validate detectors, not for model training
- **Not real-time monitoring**: Single snapshot, not time-series analysis

---

## Related Documentation

- [Implementation Plan](../plan.md) — 7 phases, acceptance criteria
- [Task Tracker](../tracker.md) — 42 tasks across 3 sprints
- [Data Contracts](data-contracts.md) — JSON schemas for detections, manifest, labels
- [Runner Documentation](runner.md) — Locator strategy, global chrome exclusions
- [QA UI Documentation](qa-ui.md) — Keyboard shortcuts, labeling workflow
- [Quality Gates](quality-gates.md) — Precision thresholds, gate definitions

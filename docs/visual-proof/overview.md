# Visual Proof Pack — Overview

## What is the Visual Proof Pack?

The **Visual Proof Pack** is a validation artifact that provides visual evidence of component detection accuracy. It consists of:

- **Annotated screenshots**: Live web pages with colored bounding boxes highlighting detected components
- **Manifest JSON files**: Structured data with bbox coordinates, locators, and component metadata
- **Human labels**: QA operator judgments (confirm/wrong/skip) for each detection

The proof pack serves as:

- **Validation checkpoint**: Verify detector precision before running full analysis

2. **Debugging tool**: Visual inspection of what the detectors actually find
3. **Documentation**: Concrete examples for stakeholders and future developers

---

## Workflow

```
DB Export → JSON → Proof Runner → Annotated Screenshots → Human QA → Labels → Regression Gates → Analysis
```

### Step 1: DB Export

Export detections from `david_component_usage` table for 5 proof pages (2 VLP + 3 Editorial).

**Input**: SQLite DB (`api/data/cu-tool.db`)
**Output**: `analysis/artifacts/visual-proof/detections.json`

### Step 2: Proof Runner (Live)

Load JSON, navigate to live URLs, query DOM using locator strategy, inject overlays, capture screenshots.

**Input**: `detections.json`
**Output**:

- `<slug>.annotated.png` (full-page screenshot with bounding boxes)
- `<slug>.manifest.json` (bbox coordinates + locator metadata)

### Step 3: Human QA

QA operator labels each detection as confirm/wrong/skip using keyboard shortcuts.

**Input**: Manifest JSON + annotated screenshots
**Output**: `labels.jsonl` (append-only, one JSON object per line)

### Step 4: Regression Gates

Compute precision per component type, validate against thresholds, block analysis if gates fail.

**Input**: `labels.jsonl`
**Output**: `regression-report.md` (pass/fail per gate)

---

## Inputs

### From Database

- **URL inventory**: 5 proof pages filtered from `url_inventory` (UK market, fetched successfully)
- **Component detections**: Per-URL detections from `david_component_usage` table
- **Evidence strings**: Parsed into structured data (item counts, selectors, controls)

### Page Lists

- `analysis/visual-proof/pages.vlp.txt`: 2 VLP URLs (Juke, Ariya)
- `analysis/visual-proof/pages.editorial.txt`: 3 Editorial URLs (Electric Vehicles, Ownership, Pop-up Event)

### Locator Strategy

- Component-specific CSS selector patterns (documented in `runner.md`)
- Global chrome exclusion rules (header, nav, footer, dialogs, OneTrust)

---

## Outputs

### Artifacts Directory Structure

```
analysis/artifacts/visual-proof/
├── detections.json           # DB export (5 URLs, all component instances)
├── full/                     # Proof runner outputs
│   ├── juke/
│   │   ├── juke.annotated.png
│   │   └── juke.manifest.json
│   ├── ariya/
│   ├── electric-vehicles/
│   ├── ownership/
│   └── pop-up-event/
├── labels.jsonl              # Human QA labels (append-only)
├── regression-report.md      # Gate validation results
└── detector-changes.md       # Before/after hardening diff
```

### What is Committed to Repo

✅ **Committed**:

- `detections.json` (< 50KB)
- `*.manifest.json` (< 5KB each)
- `labels.jsonl` (< 20KB)
- `regression-report.md`, `detector-changes.md`

❌ **NOT Committed** (local-only, .gitignore'd):

- `*.annotated.png` (screenshots, ~500KB–2MB each)
- SQLite DB (`api/data/cu-tool.db`)
- HTML cache (`api/data/html-cache/`)

Rationale: Screenshots are large binary files unsuitable for git. They can be regenerated by re-running the proof runner.

---

## Key Properties

### Deterministic

- Fixed viewport (1920x1080)
- Fixed component list (from detections.json, not heuristic re-detection)
- Fixed locator strategy (documented, versioned)

### Reproducible

- Screenshots can be regenerated by re-running runner with same inputs
- JSON artifacts are deterministic (no random UUIDs, timestamps are for metadata only)

### Auditable

- Every detection has visual evidence (screenshot) + structured data (manifest JSON)
- Human labels have timestamps and are append-only (no edits, no deletes)

### Transparent

- Locator strategy documented (no black box)
- Bbox coordinates in document coordinate system (reproducible)
- Global chrome exclusions explicit (header/nav/footer/dialogs)

---

## Non-Goals

- **Not a full-site audit**: Proof pack covers 5 pages only, not all 8,468 URLs
- **Not automated regression testing**: Screenshots are for human QA, not pixel-perfect comparison
- **Not ML training data**: Labels validate detectors, not for model training
- **Not real-time monitoring**: Single snapshot, not time-series analysis

---

## Related Documentation

- [Implementation Plan](../plan.md) — 7 phases, acceptance criteria
- [Task Tracker](../tracker.md) — 42 tasks across 3 sprints
- [Data Contracts](data-contracts.md) — JSON schemas for detections, manifest, labels
- [Runner Documentation](runner.md) — Locator strategy, global chrome exclusions
- [QA UI Documentation](qa-ui.md) — Keyboard shortcuts, labeling workflow
- [Quality Gates](quality-gates.md) — Precision thresholds, gate definitions

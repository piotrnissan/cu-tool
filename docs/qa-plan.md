# Human QA Plan — Component Detection (v1)

## 1. Purpose

Human QA validates and calibrates component detection outputs by reviewing visual candidates produced by the pipeline. The goal is to assess precision/recall trade-offs, identify false positives/negatives, and generate actionable feedback for iterative hardening—without changing detectors during QA.

## 2. Scope (QA v1)

This QA round focuses on a limited, representative UK set to validate end‑to‑end behavior (API → proof runner → visual review) prior to scaling.

### 2.1 Pages

- Homepage (UK)
- 2–3 Vehicle Landing Pages (VLP)
- 2–3 Editorial / campaign pages

### 2.2 Component Types (v1)

All v1 component keys currently emitted by the API (e.g., hero, banner/promo_section, media_text_split, cards_section, image_carousel, card_carousel, icon_grid, info_specs, next_action_panel, anchor_nav, tabs, accordion).

## 3. Non-Goals

- No detector logic changes during QA
- No attempt to force Found == Expected
- No taxonomy expansion or renaming in this phase
- No UI/UX polish beyond what is required for review

## 4. Inputs

QA operates on three interconnected artifacts:

- **Detection output**: Exported component detections from the API (`detections.json`), containing component keys, instance counts, and locator metadata per page.
- **Visual proof artifacts**: Annotated screenshots with bounding boxes overlaid on each detected instance, generated by the proof runner. Each instance is numbered and visually highlighted.
- **QA baseline**: Design-POV reference screenshots and notes documenting expected component presence and patterns for the proof pack pages (see `analysis/qa-baseline/v1-uk/`).

## 5. QA Workflow

For each page in the proof pack:

1. Load the annotated screenshot showing all detected instances with bounding boxes.
2. Review each numbered bounding box sequentially.
3. For each instance, make a decision based on visual reality (what is actually present on the page).
4. Record the decision with optional notes for edge cases or clarifications.
5. Advance to the next instance until all detections for the page are reviewed.
6. Move to the next page and repeat.

### 5.1 Review Unit

A single QA decision applies to **one detected instance** (one bounding box) on one page. Each instance has:

- A unique identifier (page URL + component key + instance number)
- A visible bounding box on the screenshot
- An associated component type assigned by the detector

### 5.2 Decision Types

Each instance is classified into one of the following:

- **Correct**: The bounding box accurately captures a real instance of the assigned component type.
- **Wrong type**: The bounding box captures a real component, but the assigned type is incorrect (specify correct type).
- **False positive**: The bounding box does not capture a meaningful component (noise, duplicate, or misfire).
- **Missing**: A component is visually present on the page but was not detected (note location and type).
- **Unclear / edge case**: Ambiguous or borderline cases requiring discussion or refinement of definitions.

## 6. Output

QA produces **structured human decisions** for each reviewed instance, containing:

- Page identifier (URL or page key)
- Component type (as assigned by the detector)
- Instance identifier (e.g., bounding box number)
- Decision type (Correct / Wrong type / False positive / Missing / Unclear)
- Corrected type (if "Wrong type" was selected)
- Optional notes (for edge cases, ambiguities, or context)

This data is used for:

- **Precision analysis**: Calculating accuracy of detections per component type.
- **Feedback loop**: Identifying systematic errors or gaps for future detector refinement.
- **Baseline validation**: Confirming alignment between QA decisions and design-POV expectations.

## 7. Rules & Principles

- **QA validates candidates, not ground truth**: QA reviews what the detector found. If nothing was detected, that outcome is recorded (not fabricated).
- **Missing detections are acceptable**: Low recall is expected and documented. QA records "Missing" instances but does not require 100% coverage.
- **No detector changes during QA**: Detectors are frozen. QA produces feedback only; fixes come later.
- **Visual reality > DOM semantics**: Decisions are based on what is visually present and meaningful to users, not HTML structure or ARIA roles.
- **Consistency over perfection**: Aim for repeatable, consistent judgments. Edge cases are explicitly marked "Unclear" rather than forced into incorrect categories.
- **One decision per instance**: Each bounding box receives exactly one classification. Do not merge or split instances during review.

## 8. Iteration Loop

<!-- How QA feedback is fed back into detector improvements -->

## 9. Exit Criteria

<!-- When this QA phase is considered complete -->

## 10. Open Questions

<!-- Parked questions, explicitly not resolved yet -->
